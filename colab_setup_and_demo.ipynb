{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# ü§ñ VERL + LangGraph Multi-Agent Coding Framework - Colab Setup\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/multiminddev/coding-framework/blob/main/colab_setup_and_demo.ipynb)\n",
    "\n",
    "This notebook sets up and demonstrates the VERL + LangGraph Multi-Agent Coding Framework in Google Colab with HuggingFace models for local inference.\n",
    "\n",
    "## Features Demonstrated:\n",
    "- üîß **Code Generator Agent**: Creates code solutions from problem descriptions\n",
    "- üìù **Code Reviewer Agent**: Reviews code for quality, security, and performance\n",
    "- ‚ö° **Code Executor Agent**: Tests and validates code execution\n",
    "- üîÑ **LangGraph Orchestration**: Multi-agent workflow coordination\n",
    "- üß† **HuggingFace Models**: Local model inference without API keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## üöÄ Setup and Installation\n",
    "\n",
    "Let's start by setting up the environment and installing dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive to save models and results\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directories for our project\n",
    "import os\n",
    "os.makedirs('/content/drive/MyDrive/coding_framework', exist_ok=True)\n",
    "os.makedirs('/content/model_cache', exist_ok=True)\n",
    "os.makedirs('/content/data', exist_ok=True)\n",
    "\n",
    "print(\"‚úÖ Google Drive mounted and directories created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "system_info"
   },
   "outputs": [],
   "source": [
    "# Check system resources\n",
    "!nvidia-smi\n",
    "!free -h\n",
    "!df -h\n",
    "\n",
    "import torch\n",
    "print(f\"\\nüî• CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/multiminddev/coding-framework.git /content/coding-framework\n",
    "%cd /content/coding-framework\n",
    "\n",
    "print(\"‚úÖ Repository cloned successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_dependencies"
   },
   "outputs": [],
   "source": [
    "# Install Colab-optimized dependencies\n",
    "!pip install -r requirements-colab.txt\n",
    "\n",
    "# Install the framework in development mode\n",
    "!pip install -e .\n",
    "\n",
    "print(\"‚úÖ Dependencies installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_environment"
   },
   "outputs": [],
   "source": [
    "# Set up environment variables for Colab\n",
    "import os\n",
    "os.environ['PYTHONPATH'] = '/content/coding-framework/src'\n",
    "os.environ['COLAB_MODE'] = 'true'\n",
    "os.environ['HF_HOME'] = '/content/model_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/content/model_cache'\n",
    "\n",
    "# Optional: Set HuggingFace token if you want to access gated models\n",
    "# os.environ['HUGGINGFACE_HUB_TOKEN'] = 'your_token_here'\n",
    "\n",
    "print(\"‚úÖ Environment variables configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_selection"
   },
   "source": [
    "## üß† Model Selection for Colab\n",
    "\n",
    "Choose the appropriate model based on your Colab resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "select_model"
   },
   "outputs": [],
   "source": [
    "# Model options for different Colab tiers\n",
    "MODELS = {\n",
    "    \"lightweight\": {\n",
    "        \"name\": \"microsoft/DialoGPT-small\",\n",
    "        \"description\": \"Lightweight model, works on free Colab\",\n",
    "        \"memory\": \"~1GB\"\n",
    "    },\n",
    "    \"medium\": {\n",
    "        \"name\": \"microsoft/CodeGPT-small-py\", \n",
    "        \"description\": \"Code-focused model, good balance\",\n",
    "        \"memory\": \"~2GB\"\n",
    "    },\n",
    "    \"advanced\": {\n",
    "        \"name\": \"Salesforce/codegen-350M-multi\",\n",
    "        \"description\": \"Multi-language code generation\",\n",
    "        \"memory\": \"~3GB\"\n",
    "    },\n",
    "    \"powerful\": {\n",
    "        \"name\": \"bigcode/starcoder2-3b\",\n",
    "        \"description\": \"High-quality code generation (Colab Pro)\",\n",
    "        \"memory\": \"~6GB\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display model options\n",
    "print(\"Available models for Colab:\")\n",
    "for key, model in MODELS.items():\n",
    "    print(f\"\\n{key.upper()}:\")\n",
    "    print(f\"  Model: {model['name']}\")\n",
    "    print(f\"  Description: {model['description']}\")\n",
    "    print(f\"  Memory: {model['memory']}\")\n",
    "\n",
    "# Select model based on GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    if gpu_memory > 14:  # Colab Pro/Pro+\n",
    "        selected_model = MODELS[\"powerful\"]\n",
    "    elif gpu_memory > 10:  # Standard GPU\n",
    "        selected_model = MODELS[\"advanced\"]\n",
    "    else:\n",
    "        selected_model = MODELS[\"medium\"]\n",
    "else:\n",
    "    selected_model = MODELS[\"lightweight\"]\n",
    "\n",
    "print(f\"\\nüéØ Selected model: {selected_model['name']}\")\n",
    "print(f\"üìù Description: {selected_model['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "framework_setup"
   },
   "source": [
    "## ‚öôÔ∏è Framework Configuration\n",
    "\n",
    "Set up the framework with Colab-optimized configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_config"
   },
   "outputs": [],
   "source": "# Create Colab-specific configuration\nfrom src.coding_framework.utils.config import Config, LLMConfig\nimport yaml\n\n# Load base Colab config\nwith open('config/colab_config.yaml', 'r') as f:\n    config_data = yaml.safe_load(f)\n\n# Update with selected model\nconfig_data['llm']['model'] = selected_model['name']\n\n# Remove Colab-specific settings that aren't part of the Config model\nif 'colab' in config_data:\n    print(f\"üóëÔ∏è Removing Colab-specific settings: {list(config_data['colab'].keys())}\")\n    del config_data['colab']\n\n# Save updated config\nwith open('/content/colab_runtime_config.yaml', 'w') as f:\n    yaml.dump(config_data, f, default_flow_style=False)\n\n# Load configuration\nconfig = Config(**config_data)\n\nprint(\"‚úÖ Configuration created for Colab\")\nprint(f\"üß† Using model: {config.llm.model}\")\nprint(f\"üîß Provider: {config.llm.provider}\")\nprint(f\"üéõÔ∏è Max tokens: {config.llm.max_tokens}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "initialize_framework"
   },
   "outputs": [],
   "source": "# Initialize the framework\nimport asyncio\nfrom src.coding_framework.orchestration import CodingSupervisor\nfrom src.coding_framework.utils import setup_logging\nfrom src.coding_framework.utils.llm_interface import LLMInterface\nfrom transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\nimport torch\n\n# Setup logging for Colab\nsetup_logging(level=\"INFO\", verbose=True, format_type=\"text\")\n\n# Initialize supervisor\nsupervisor = CodingSupervisor(config)\n\nprint(\"üéØ Initializing framework components...\")\nprint(\"‚ö†Ô∏è  This may take a few minutes to download and load the model...\")\n\n# Modify the _initialize_huggingface_model method to fix the device argument issue\nasync def _initialize_huggingface_model_fixed(self):\n    \"\"\"Initialize HuggingFace model for local inference.\"\"\"\n    try:\n        import torch\n        from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n        from langchain_huggingface import HuggingFacePipeline\n        \n        self.logger.info(\"Loading HuggingFace model\", model=self.config.model)\n        \n        # Determine device\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.logger.info(\"Using device\", device=device)\n        \n        # Load tokenizer and model\n        tokenizer = AutoTokenizer.from_pretrained(\n            self.config.model,\n            trust_remote_code=True,\n        )\n        \n        # Add padding token if missing\n        if tokenizer.pad_token is None:\n            tokenizer.pad_token = tokenizer.eos_token\n        \n        model = AutoModelForCausalLM.from_pretrained(\n            self.config.model,\n            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n            device_map=\"auto\" if device == \"cuda\" else None,\n            trust_remote_code=True,\n        )\n        \n        # Create pipeline - remove max_new_tokens to avoid conflicts\n        pipe = pipeline(\n            \"text-generation\",\n            model=model,\n            tokenizer=tokenizer,\n            do_sample=True,\n            return_full_text=False,\n        )\n        \n        # Wrap in LangChain\n        hf_llm = HuggingFacePipeline(pipeline=pipe)\n        \n        # Store model info for cleanup\n        self.hf_model = model\n        self.hf_tokenizer = tokenizer\n        self.hf_pipeline = pipe\n        \n        self.logger.info(\"HuggingFace model loaded successfully\")\n        return hf_llm\n        \n    except Exception as e:\n        self.logger.error(\"Failed to initialize HuggingFace model\", error=str(e))\n        raise\n\n# Fix the _test_connection method to handle HuggingFace response format\nasync def _test_connection_fixed(self):\n    \"\"\"Test connection to LLM provider.\"\"\"\n    try:\n        from langchain_core.messages import HumanMessage, SystemMessage\n        \n        test_messages = [\n            SystemMessage(content=\"You are a helpful assistant.\"),\n            HumanMessage(content=\"Say 'OK' if you can hear me.\"),\n        ]\n        \n        response = await self.client.ainvoke(test_messages)\n        \n        # Handle different response formats\n        if hasattr(response, 'content'):\n            response_content = response.content\n        else:\n            # For HuggingFace models, response might be a string directly\n            response_content = str(response)\n        \n        self.logger.info(\"Connection test successful\", \n                       response_length=len(response_content))\n        \n    except Exception as e:\n        self.logger.error(\"Connection test failed\", error=str(e))\n        raise\n\n# Fix the call method to handle max_tokens parameter conflicts\nasync def call_fixed(self, messages, temperature=None, max_tokens=None, **kwargs):\n    \"\"\"Fixed call method that avoids max_tokens conflicts with HuggingFace.\"\"\"\n    if not self.is_initialized:\n        await self.initialize()\n    \n    # Rate limiting\n    await self._rate_limit()\n    \n    import time\n    start_time = time.time()\n    \n    try:\n        # Remove conflicting parameters from kwargs\n        clean_kwargs = {k: v for k, v in kwargs.items() if k not in ['max_tokens', 'temperature']}\n        \n        # For HuggingFace models, handle parameters differently\n        if self.config.provider == \"huggingface\":\n            # Use the custom HuggingFace call method\n            response = await self._call_huggingface_model(messages, {\n                'max_tokens': max_tokens or self.config.max_tokens,\n                'temperature': temperature or self.config.temperature,\n                **clean_kwargs\n            })\n            content = response\n        else:\n            # For other providers, use the original logic\n            call_kwargs = {}\n            \n            if temperature is not None:\n                if hasattr(self.client, 'temperature'):\n                    self.client.temperature = temperature\n                else:\n                    call_kwargs['temperature'] = temperature\n            \n            if max_tokens is not None:\n                if hasattr(self.client, 'max_tokens'):\n                    self.client.max_tokens = max_tokens\n                else:\n                    call_kwargs['max_tokens'] = max_tokens\n            \n            call_kwargs.update(clean_kwargs)\n            \n            if call_kwargs:\n                response = await self.client.ainvoke(messages, **call_kwargs)\n            else:\n                response = await self.client.ainvoke(messages)\n            \n            content = response.content if hasattr(response, 'content') else str(response)\n        \n        response_time = time.time() - start_time\n        \n        self.logger.info(\"LLM call successful\",\n                       response_time=response_time,\n                       response_length=len(content))\n        \n        return content\n        \n    except Exception as e:\n        response_time = time.time() - start_time\n        self.logger.error(\"LLM call failed\",\n                        error=str(e),\n                        response_time=response_time)\n        raise\n\n# Fix BaseAgent _call_llm method to avoid duplicate parameters\nfrom src.coding_framework.agents.base_agent import BaseAgent\n\nasync def _call_llm_fixed(self, messages, **kwargs):\n    \"\"\"Fixed _call_llm method that avoids parameter conflicts.\"\"\"\n    import time\n    start_time = time.time()\n    \n    try:\n        # Extract parameters to avoid duplication\n        temperature = kwargs.pop(\"temperature\", self.config.temperature)\n        max_tokens = kwargs.pop(\"max_tokens\", self.config.max_tokens)\n        \n        response = await self.llm_interface.call(\n            messages=messages,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            **kwargs,\n        )\n        \n        execution_time = time.time() - start_time\n        \n        self.logger.info(\n            \"LLM call successful\",\n            execution_time=execution_time,\n            response_length=len(response),\n        )\n        \n        # Update performance metrics\n        self.performance_metrics[\"last_llm_call_time\"] = execution_time\n        self.performance_metrics[\"total_llm_calls\"] = (\n            self.performance_metrics.get(\"total_llm_calls\", 0) + 1\n        )\n        \n        return response\n        \n    except Exception as e:\n        execution_time = time.time() - start_time\n        self.logger.error(\n            \"LLM call failed\",\n            error=str(e),\n            execution_time=execution_time,\n        )\n        raise\n\n# Temporarily replace the original methods with the fixed ones\noriginal_initialize_huggingface_model = LLMInterface._initialize_huggingface_model\noriginal_test_connection = LLMInterface._test_connection\noriginal_call = LLMInterface.call\noriginal_call_llm = BaseAgent._call_llm\n\nLLMInterface._initialize_huggingface_model = _initialize_huggingface_model_fixed\nLLMInterface._test_connection = _test_connection_fixed\nLLMInterface.call = call_fixed\nBaseAgent._call_llm = _call_llm_fixed\n\n# Initialize in async context\nasync def init_framework():\n    try:\n        # Initialize supervisor (which includes LLM interface)\n        await supervisor.initialize()\n        return await supervisor.health_check()\n    finally:\n        # Restore the original methods after initialization\n        LLMInterface._initialize_huggingface_model = original_initialize_huggingface_model\n        LLMInterface._test_connection = original_test_connection\n        LLMInterface.call = original_call\n        BaseAgent._call_llm = original_call_llm\n\n# Run initialization\nhealth_status = await init_framework()\n\nprint(\"\\n‚úÖ Framework initialized successfully!\")\nprint(f\"üè• System health: {health_status['system']['status']}\")\n\n# Print detailed health status\nif health_status['system']['status'] == 'unhealthy':\n    print(\"\\nüîç Health Check Details:\")\n    for component, status in health_status.items():\n        if isinstance(status, dict) and 'status' in status:\n            print(f\"  {component}: {status['status']}\")\n            if status['status'] == 'unhealthy' and 'error' in status:\n                print(f\"    Error: {status['error']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "demo_section"
   },
   "source": [
    "## üéØ Demo: Multi-Agent Code Generation\n",
    "\n",
    "Let's demonstrate the framework with a complete coding workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "demo_simple_problem"
   },
   "outputs": [],
   "source": [
    "# Demo 1: Simple problem solving\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.syntax import Syntax\n",
    "\n",
    "console = Console()\n",
    "\n",
    "async def demo_simple_problem():\n",
    "    problem = \"Write a Python function to calculate the factorial of a number\"\n",
    "    \n",
    "    console.print(Panel.fit(\n",
    "        f\"[bold blue]Problem:[/bold blue] {problem}\",\n",
    "        border_style=\"blue\"\n",
    "    ))\n",
    "    \n",
    "    # Solve the problem\n",
    "    result = await supervisor.solve_problem(\n",
    "        problem,\n",
    "        context={\n",
    "            \"language\": \"python\",\n",
    "            \"style\": \"clean\",\n",
    "            \"include_tests\": False\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if result[\"success\"]:\n",
    "        # Display generated code\n",
    "        console.print(\"\\n[bold green]‚úÖ Generated Code:[/bold green]\")\n",
    "        syntax = Syntax(result[\"code\"], \"python\", theme=\"github-dark\")\n",
    "        console.print(syntax)\n",
    "        \n",
    "        # Display review\n",
    "        if result.get(\"review\"):\n",
    "            console.print(\"\\n[bold yellow]üìù Code Review:[/bold yellow]\")\n",
    "            console.print(result[\"review\"])\n",
    "        \n",
    "        # Display execution results\n",
    "        if result.get(\"execution\"):\n",
    "            console.print(\"\\n[bold cyan]‚ö° Execution Results:[/bold cyan]\")\n",
    "            console.print(result[\"execution\"])\n",
    "        \n",
    "        console.print(f\"\\n[dim]‚è±Ô∏è  Total time: {result.get('execution_time', 0):.2f}s[/dim]\")\n",
    "    else:\n",
    "        console.print(f\"[bold red]‚ùå Error:[/bold red] {result.get('error')}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run the demo\n",
    "simple_result = await demo_simple_problem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "demo_complex_problem"
   },
   "outputs": [],
   "source": [
    "# Demo 2: More complex problem with multiple iterations\n",
    "async def demo_complex_problem():\n",
    "    problem = \"\"\"Create a Python class that implements a simple calculator with the following features:\n",
    "    1. Basic operations (add, subtract, multiply, divide)\n",
    "    2. Memory functions (store, recall, clear)\n",
    "    3. Error handling for division by zero\n",
    "    4. Method chaining support\n",
    "    \"\"\"\n",
    "    \n",
    "    console.print(Panel.fit(\n",
    "        f\"[bold blue]Complex Problem:[/bold blue]\\n{problem}\",\n",
    "        border_style=\"blue\"\n",
    "    ))\n",
    "    \n",
    "    # Solve with enhanced context\n",
    "    result = await supervisor.solve_problem(\n",
    "        problem,\n",
    "        context={\n",
    "            \"language\": \"python\",\n",
    "            \"style\": \"clean\",\n",
    "            \"include_tests\": False,\n",
    "            \"max_iterations\": 3\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if result[\"success\"]:\n",
    "        console.print(\"\\n[bold green]‚úÖ Generated Calculator Class:[/bold green]\")\n",
    "        syntax = Syntax(result[\"code\"], \"python\", theme=\"github-dark\")\n",
    "        console.print(syntax)\n",
    "        \n",
    "        console.print(f\"\\n[bold magenta]üìä Metrics:[/bold magenta]\")\n",
    "        console.print(f\"‚Ä¢ Iterations: {result.get('iterations', 'N/A')}\")\n",
    "        console.print(f\"‚Ä¢ Review Score: {result.get('review_score', 'N/A')}/100\")\n",
    "        console.print(f\"‚Ä¢ Execution Success: {result.get('execution_success', 'N/A')}\")\n",
    "    else:\n",
    "        console.print(f\"[bold red]‚ùå Error:[/bold red] {result.get('error')}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run the complex demo\n",
    "complex_result = await demo_complex_problem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "demo_individual_agents"
   },
   "outputs": [],
   "source": [
    "# Demo 3: Individual agent testing\n",
    "async def test_individual_agents():\n",
    "    console.print(Panel.fit(\n",
    "        \"[bold purple]Testing Individual Agents[/bold purple]\",\n",
    "        border_style=\"purple\"\n",
    "    ))\n",
    "    \n",
    "    # Test Code Generator\n",
    "    console.print(\"\\n[bold]1. Testing Code Generator Agent:[/bold]\")\n",
    "    gen_result = await supervisor.generate_code(\n",
    "        \"Write a function to check if a number is prime\",\n",
    "        context={\"language\": \"python\"}\n",
    "    )\n",
    "    \n",
    "    if gen_result[\"success\"]:\n",
    "        console.print(\"[green]‚úÖ Generator working[/green]\")\n",
    "        print(f\"Generated: {len(gen_result['content'])} characters\")\n",
    "    else:\n",
    "        console.print(\"[red]‚ùå Generator failed[/red]\")\n",
    "    \n",
    "    # Test Code Reviewer\n",
    "    console.print(\"\\n[bold]2. Testing Code Reviewer Agent:[/bold]\")\n",
    "    sample_code = '''def factorial(n):\n",
    "    if n == 0:\n",
    "        return 1\n",
    "    return n * factorial(n-1)'''\n",
    "    \n",
    "    review_result = await supervisor.review_code(sample_code)\n",
    "    \n",
    "    if review_result[\"success\"]:\n",
    "        console.print(\"[green]‚úÖ Reviewer working[/green]\")\n",
    "        score = review_result.get(\"metadata\", {}).get(\"overall_score\", \"N/A\")\n",
    "        print(f\"Review score: {score}/100\")\n",
    "    else:\n",
    "        console.print(\"[red]‚ùå Reviewer failed[/red]\")\n",
    "    \n",
    "    # Test Code Executor\n",
    "    console.print(\"\\n[bold]3. Testing Code Executor Agent:[/bold]\")\n",
    "    simple_code = \"print('Hello from the executor!')\"\n",
    "    \n",
    "    exec_result = await supervisor.execute_code(\n",
    "        simple_code,\n",
    "        context={\"language\": \"python\"}\n",
    "    )\n",
    "    \n",
    "    if exec_result[\"success\"]:\n",
    "        console.print(\"[green]‚úÖ Executor working[/green]\")\n",
    "    else:\n",
    "        console.print(\"[red]‚ùå Executor failed[/red]\")\n",
    "        console.print(f\"Error: {exec_result.get('error')}\")\n",
    "\n",
    "# Test individual agents\n",
    "await test_individual_agents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "performance_section"
   },
   "source": [
    "## üìä Performance Monitoring\n",
    "\n",
    "Monitor system performance during execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "monitor_performance"
   },
   "outputs": [],
   "source": [
    "# Performance monitoring\n",
    "import psutil\n",
    "import GPUtil\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_system_stats():\n",
    "    \"\"\"Get current system statistics.\"\"\"\n",
    "    stats = {\n",
    "        'cpu_percent': psutil.cpu_percent(interval=1),\n",
    "        'memory_percent': psutil.virtual_memory().percent,\n",
    "        'disk_percent': psutil.disk_usage('/').percent\n",
    "    }\n",
    "    \n",
    "    # GPU stats if available\n",
    "    try:\n",
    "        gpus = GPUtil.getGPUs()\n",
    "        if gpus:\n",
    "            stats['gpu_percent'] = gpus[0].load * 100\n",
    "            stats['gpu_memory_percent'] = (gpus[0].memoryUsed / gpus[0].memoryTotal) * 100\n",
    "    except:\n",
    "        stats['gpu_percent'] = 0\n",
    "        stats['gpu_memory_percent'] = 0\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Get current performance metrics\n",
    "stats = get_system_stats()\n",
    "performance_metrics = supervisor.get_performance_metrics()\n",
    "\n",
    "console.print(Panel.fit(\n",
    "    f\"\"\"[bold cyan]System Performance[/bold cyan]\n",
    "\n",
    "[bold]Resource Usage:[/bold]\n",
    "üî• CPU: {stats['cpu_percent']:.1f}%\n",
    "üß† RAM: {stats['memory_percent']:.1f}%\n",
    "üíæ Disk: {stats['disk_percent']:.1f}%\n",
    "üéÆ GPU: {stats['gpu_percent']:.1f}%\n",
    "üìä GPU Memory: {stats['gpu_memory_percent']:.1f}%\n",
    "\n",
    "[bold]Framework Metrics:[/bold]\n",
    "‚úÖ Problems Solved: {performance_metrics.get('total_problems_solved', 0)}\n",
    "üéØ Success Rate: {(performance_metrics.get('successful_solutions', 0) / max(1, performance_metrics.get('total_problems_solved', 1)) * 100):.1f}%\n",
    "‚è±Ô∏è  Avg Response Time: {performance_metrics.get('avg_execution_time', 0):.2f}s\n",
    "üìù Avg Review Score: {performance_metrics.get('avg_review_score', 0):.1f}/100\n",
    "\"\"\",\n",
    "    border_style=\"cyan\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "testing_section"
   },
   "source": [
    "## üß™ Running Tests\n",
    "\n",
    "Let's run some basic tests to ensure everything works correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_basic_tests"
   },
   "outputs": [],
   "source": [
    "# Run basic framework tests\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "console.print(Panel.fit(\n",
    "    \"[bold green]Running Framework Tests[/bold green]\",\n",
    "    border_style=\"green\"\n",
    "))\n",
    "\n",
    "# Test configuration loading\n",
    "print(\"1. Testing configuration loading...\")\n",
    "try:\n",
    "    from src.coding_framework.utils.config import load_config\n",
    "    test_config = load_config('/content/colab_runtime_config.yaml')\n",
    "    assert test_config.llm.provider == \"huggingface\"\n",
    "    print(\"   ‚úÖ Configuration test passed\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Configuration test failed: {e}\")\n",
    "\n",
    "# Test LLM interface\n",
    "print(\"\\n2. Testing LLM interface...\")\n",
    "try:\n",
    "    from src.coding_framework.utils.llm_interface import LLMInterface\n",
    "    llm_interface = LLMInterface(test_config.llm)\n",
    "    health = await llm_interface.health_check()\n",
    "    print(f\"   ‚úÖ LLM interface test passed: {health['status']}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå LLM interface test failed: {e}\")\n",
    "\n",
    "# Test agent initialization\n",
    "print(\"\\n3. Testing agent initialization...\")\n",
    "try:\n",
    "    health_status = await supervisor.health_check()\n",
    "    healthy_agents = sum(1 for agent, status in health_status.items() \n",
    "                        if isinstance(status, dict) and status.get('status') == 'healthy')\n",
    "    print(f\"   ‚úÖ Agent initialization test passed: {healthy_agents} healthy agents\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Agent initialization test failed: {e}\")\n",
    "\n",
    "# Test basic workflow\n",
    "print(\"\\n4. Testing basic workflow...\")\n",
    "try:\n",
    "    simple_test = await supervisor.solve_problem(\n",
    "        \"Write a function that returns 'Hello World'\",\n",
    "        context={\"language\": \"python\", \"max_iterations\": 1}\n",
    "    )\n",
    "    if simple_test[\"success\"]:\n",
    "        print(\"   ‚úÖ Basic workflow test passed\")\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è  Basic workflow test completed with issues: {simple_test.get('error')}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Basic workflow test failed: {e}\")\n",
    "\n",
    "print(\"\\nüéâ Test suite completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_unit_tests"
   },
   "outputs": [],
   "source": [
    "# Run selected unit tests (modified for Colab)\n",
    "print(\"Running unit tests (Colab-compatible subset)...\")\n",
    "\n",
    "# Test imports\n",
    "try:\n",
    "    from src.coding_framework import (\n",
    "        CodeGeneratorAgent,\n",
    "        CodeReviewerAgent,\n",
    "        CodeExecutorAgent,\n",
    "        CodingSupervisor\n",
    "    )\n",
    "    print(\"‚úÖ All core imports successful\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "\n",
    "# Test agent type properties\n",
    "try:\n",
    "    generator = supervisor.agents[\"generator\"]\n",
    "    reviewer = supervisor.agents[\"reviewer\"]\n",
    "    executor = supervisor.agents[\"executor\"]\n",
    "    \n",
    "    assert generator.agent_type == \"code_generator\"\n",
    "    assert reviewer.agent_type == \"code_reviewer\"\n",
    "    assert executor.agent_type == \"code_executor\"\n",
    "    \n",
    "    print(\"‚úÖ Agent type tests passed\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Agent type test failed: {e}\")\n",
    "\n",
    "# Test configuration validation\n",
    "try:\n",
    "    from src.coding_framework.utils.config import validate_config\n",
    "    issues = validate_config(test_config)\n",
    "    if not issues:\n",
    "        print(\"‚úÖ Configuration validation passed\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Configuration has issues: {issues}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Configuration validation failed: {e}\")\n",
    "\n",
    "print(\"\\nüß™ Unit tests completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "save_results"
   },
   "source": [
    "## üíæ Save Results and Models\n",
    "\n",
    "Save your work to Google Drive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_to_drive"
   },
   "outputs": [],
   "source": [
    "# Save results and configuration to Drive\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Prepare results summary\n",
    "results_summary = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model_used\": selected_model[\"name\"],\n",
    "    \"system_info\": {\n",
    "        \"gpu_available\": torch.cuda.is_available(),\n",
    "        \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\",\n",
    "        \"colab_type\": \"GPU\" if torch.cuda.is_available() else \"CPU\"\n",
    "    },\n",
    "    \"performance_metrics\": supervisor.get_performance_metrics(),\n",
    "    \"demo_results\": {\n",
    "        \"simple_problem\": {\n",
    "            \"success\": simple_result.get(\"success\", False),\n",
    "            \"execution_time\": simple_result.get(\"execution_time\", 0)\n",
    "        },\n",
    "        \"complex_problem\": {\n",
    "            \"success\": complex_result.get(\"success\", False),\n",
    "            \"execution_time\": complex_result.get(\"execution_time\", 0),\n",
    "            \"iterations\": complex_result.get(\"iterations\", 0)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to Drive\n",
    "output_dir = '/content/drive/MyDrive/coding_framework/colab_session'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save results\n",
    "with open(f'{output_dir}/results_{timestamp}.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "# Save configuration\n",
    "with open(f'{output_dir}/config_{timestamp}.yaml', 'w') as f:\n",
    "    yaml.dump(config.dict(), f, default_flow_style=False)\n",
    "\n",
    "# Save demo code examples\n",
    "if simple_result.get(\"success\"):\n",
    "    with open(f'{output_dir}/generated_code_{timestamp}.py', 'w') as f:\n",
    "        f.write(f\"# Simple Problem Solution\\n\")\n",
    "        f.write(f\"# Generated at: {datetime.now()}\\n\")\n",
    "        f.write(f\"# Model: {selected_model['name']}\\n\\n\")\n",
    "        f.write(simple_result[\"code\"])\n",
    "        \n",
    "        if complex_result.get(\"success\"):\n",
    "            f.write(f\"\\n\\n# Complex Problem Solution\\n\")\n",
    "            f.write(complex_result[\"code\"])\n",
    "\n",
    "console.print(Panel.fit(\n",
    "    f\"\"\"[bold green]‚úÖ Session Saved Successfully![/bold green]\n",
    "\n",
    "[bold]Saved Files:[/bold]\n",
    "üìä Results: results_{timestamp}.json\n",
    "‚öôÔ∏è  Config: config_{timestamp}.yaml\n",
    "üêç Code: generated_code_{timestamp}.py\n",
    "\n",
    "[bold]Location:[/bold] {output_dir}\n",
    "\n",
    "[dim]You can access these files from your Google Drive.[/dim]\n",
    "\"\"\",\n",
    "    border_style=\"green\"\n",
    "))\n",
    "\n",
    "print(f\"\\nüéØ Total problems solved: {supervisor.get_performance_metrics().get('total_problems_solved', 0)}\")\n",
    "print(f\"‚è±Ô∏è  Average response time: {supervisor.get_performance_metrics().get('avg_execution_time', 0):.2f}s\")\n",
    "print(f\"üìù Average review score: {supervisor.get_performance_metrics().get('avg_review_score', 0):.1f}/100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## üöÄ Next Steps\n",
    "\n",
    "Now that you have the framework running in Colab, here are some things you can try:\n",
    "\n",
    "### üéØ **Experiment with Different Problems**\n",
    "```python\n",
    "# Try more complex problems\n",
    "problems = [\n",
    "    \"Implement a binary search tree with insert, delete, and search operations\",\n",
    "    \"Create a web scraper that respects robots.txt\",\n",
    "    \"Write a function to find the longest common subsequence\",\n",
    "    \"Implement a simple neural network from scratch\"\n",
    "]\n",
    "\n",
    "for problem in problems:\n",
    "    result = await supervisor.solve_problem(problem, context={\"language\": \"python\"})\n",
    "    # Process results...\n",
    "```\n",
    "\n",
    "### üîß **Customize Configuration**\n",
    "- Modify `config/colab_config.yaml` for different settings\n",
    "- Try different HuggingFace models\n",
    "- Adjust workflow parameters\n",
    "\n",
    "### üß† **Try Different Models**\n",
    "- Code-specific models: `bigcode/starcoder2-3b`\n",
    "- Multi-language models: `Salesforce/codegen-350M-multi`\n",
    "- Instruction-tuned models: `microsoft/DialoGPT-medium`\n",
    "\n",
    "### üìä **Monitor Performance**\n",
    "- Use the performance monitoring cells above\n",
    "- Track memory usage with different model sizes\n",
    "- Compare generation quality across models\n",
    "\n",
    "### üöÄ **Scale Up (Colab Pro)**\n",
    "- Use larger models with more GPU memory\n",
    "- Enable VERL training (coming soon)\n",
    "- Process multiple problems in parallel\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Resources\n",
    "\n",
    "- **Framework Documentation**: [GitHub Repository](https://github.com/multiminddev/coding-framework)\n",
    "- **HuggingFace Models**: [Hub](https://huggingface.co/models?pipeline_tag=text-generation)\n",
    "- **LangGraph Documentation**: [LangChain Docs](https://langchain-ai.github.io/langgraph/)\n",
    "- **VERL Framework**: [VERL GitHub](https://github.com/volcengine/verl)\n",
    "\n",
    "## üêõ Troubleshooting\n",
    "\n",
    "If you encounter issues:\n",
    "\n",
    "1. **Memory Issues**: Use smaller models or restart runtime\n",
    "2. **Model Loading**: Check internet connection and HuggingFace status\n",
    "3. **CUDA Errors**: Restart runtime and ensure GPU is enabled\n",
    "4. **Import Errors**: Reinstall dependencies with `!pip install -r requirements-colab.txt`\n",
    "\n",
    "---\n",
    "\n",
    "**üéâ Congratulations!** You've successfully set up and tested the VERL + LangGraph Multi-Agent Coding Framework in Google Colab with HuggingFace models! üöÄ"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}